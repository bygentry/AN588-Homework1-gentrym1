---
title: "module8"
author: "Gentry Miller"
format: html
editor: visual
---

# Module 8

## Probabilities and Distributions

### Prelim

#### Packages

```{r}
#| label: Packages
#install.packages("manipulate")
library(manipulate)
```

### Probability

#### Challenge 1

```{r}
#| label: Rolling Two Dice

# copied and pasted into console stepwise
nrolls <- 1000
roll <- function(x){
  sample(1:6, x, replace=TRUE)
}
two_dice <- roll(nrolls) + roll(nrolls)
hist(two_dice, breaks=c(1.5:12.5), probability=TRUE, main="Rolling Two Dice", xlab="sum of rolls", ylab="probability")
```

##### Rules of Probability

![](images/Screenshot%202025-02-07%20141917.png)

#### Challenge 2

1.  P(face card) = 12/52 = .231
2.  P(king) = 4/52 = .077
3.  P(space) = 13/52 = .25
4.  P(spade \| face) = 3/12 = .25
5.  P(king \| face) = 4/12 = .33
6.  P(red suit and face card) =
    1.  P(red) = 26/52 = .5
    2.  P(face) = 12/52 = .231
    3.  P(red \| face) = 6/12 = .5
    4.  P(red $\bigcap$ face) = P(red \| face) x P(face) = .5 \* .231 = .115
7.  P(club or not face)
    1.  P(club) = 13/52 = .25
    2.  P(not face) = 40/52 = .769
    3.  P(club $\bigcap$ not face) = .25 \* .769 = .192
    4.  P(club $\bigcup$ not face) = (.25 + .769) - .192 = .827

### Random Variables

**random variable** - a variable whose outcomes are assumed to arise by chance or according to some random or stochastic mechanism

-   *Discrete* *random variable* - random variables that can assume only a countable number of discrete possibilities

-   *Continuous* *random variable* - random variables that can assume *any* real number value within a given range

**probability function** - mathematical function that describes the chance associated with a random variable having a particular outcome or falling within a given range of outcome values

-   *Probability mass function (PMF) -* probability function associated with discrete random variables. PMFs describe the propability that a random variable takes a particular discrete value.

    -   ![](images/Screenshot%202025-02-07%20164009.png)

```{r}
#| label: Flipping a Fair Coin

outcomes <- c("heads", "tails")
prob <- c(1/2, 1/2)
barplot(prob, ylim=c(0,0.6), names.arg=outcomes, space=.1, xlab="outcome", ylab="Pr(X = outcome)", main="Probability Mass Function")
```

```{r}
#| label: Cumulative Probability (Coin)

cumprob <- cumsum(prob)
barplot(cumprob, names.arg=outcomes, space=.1, xlab="outcome", ylab="Cumulative Pr(X)", main="Cumulative Probability")
```

```{r}
#| label: Rolling a Fair Die

outcomes <- c(1, 2, 3, 4, 5, 6)
prob <- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
barplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = "outcome",
    ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

```{r}
#| label: Cumulative Probability (Die)

cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
```

-   *Probability Density Functions (PDF)* - associated with continuous random variables. Describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.

    -   ![](images/clipboard-3311353508.png)

```{r}
#| label: Example: Beta Distribution

# The Beta Distribution refers to a family of continuous probability distributions defined over the interval [0, 1] and parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable x and control the shape of the distribution.

#f(x) = K x^(α−1) * (1−x)^(β−1)

library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from = 0, to = 1, by = 0.025)
fx <- K * x^(a - 1) * (1 - x)^(b - 1)
lower_x <- seq(from = -0.25, to = 0, by = 0.025)  # add some values of x less than zero
upper_x <- seq(from = 1, to = 1.25, by = 0.025)  # add some values of x greater than one
lower_fx <- rep(0, 11)  # add fx=0 values to x<0
upper_fx <- rep(0, 11)  # add fx=0 values to x>1
x <- c(lower_x, x, upper_x)  # paste xs together
fx <- c(lower_fx, fx, upper_fx)  # paste fxs together
d <- as.data.frame(cbind(x, fx))
p <- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```

```{r}
#| label: Example: Beta Distribution Cont.

# to show that the above example is a PDF:

library(manipulate)
manipulate(ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") +
    geom_line() + geom_polygon(data = data.frame(xvals = c(0, n, n, 0), fxvals = c(0,
    K * n^(a - 1) * (1 - n)^(b - 1), 0, 0)), aes(x = xvals, y = fxvals)) + ggtitle(paste("Area Under Function = ",
    0.5 * n * K * n^(a - 1) * (1 - n)^(b - 1), sep = " ")), n = slider(0, 1,
    initial = 0.5, step = 0.01))
```

**cumulative distribution function (CDF)** - the probability of observing a random variable X taking the value of x or less $F(x) = Pr(X \le x)$

-   This definition applies regardless of whether X is discrete or continuous. Note here we are using F(x) for the cumulative distribution function rather than f(x), which we use for the probability density function. For a continuous variable, the *PDF* is simply the first derivative of the *CDF*, i.e., $f(x) = d\:F(x)$

```{r}
#|label: Cumulative Probability

x <- seq(from = 0, to = 1, by = 0.005)
prob <- 0.5 * x * K * x^(a - 1) * (1 - x)^(b - 1)
barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x",
    ylab = "Pr(X ≤ x)")

pbeta(0.75, 2, 1)  # cumulative probability for x ≤ 0.75

pbeta(0.5, 2, 1)  # cumulative probability for x ≤ 0.50

# In general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from −inf to x

pbeta(0.7, 2, 1)  # yields .49

qbeta(0.49, 2, 1)  # yield 0.7
```

We can define the **survival function** for a random variable $X \: \text{as} \:S(x) = Pr(X \gt x) = 1 - Pr(X \le x) = 1 - f(x)$

We can define the "qth" *quantile* of a *cumulative distribution function* as the value of $x$ at which the CDF has the value "q", i.e. $F(x_q) = q$

### Expected Mean and Variance of Random Variables

The mean value and the expected variance for a random variable with a given *PMF* can be expressed generally as:

$$
\mu_X = \sum x_i \times P(X=x_i) \: \forall \: x \: \mid x_i \le x \le x_k \\
\sigma^2_X = \sum (x_i-\mu_X)^2 \times P(X=x_i) \: \forall \: x \:\mid x_i\le x \le x_k
$$

```{r}
#| label: Expectation for X for many die

# Expected Mean
m <- sum(seq(1:6) * 1/6)
#m

# Expected Variance
var <- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6))
var
```

![](images/clipboard-555390688.png)

### Useful Probability Mass Functions

#### The Bernoulli Distribution

The Bernoulli Distribution is the proability distribution of a *binary* random variable. If $p$ is the probability of one outcome, then $1 - p$ has to be the probability of the alternative.

For the Bernoulli distribution, the PMF is:

$$
f(x) = p^x(1-p)^{1-x}\: \text{where}\: x=\{0 \: \text{or} \: 1\}
$$

For this distribution, $\mu_X = p$ and $\sigma^2_X = p(1-p)$

### Challenge 3

![](images/clipboard-4088256972.png)

$P(spade) = (13/52)^1 \times (39/52)^0 = 0.25$

$Var(spade) = (13/52) \times (1-13/52) = (0.25) \times (0.75) = 0.1875$

#### Binomial Distribution

The Bernoulli distribution is a special case of the **Binomial Distribution**.

For the Binomial Distribution, the PMF is:

$$
f(x) = \:^nC_k \: p^k (1-p)^{n-k}
$$

where x = {0, 1, 2, ..., n} and where

$$
^nC_k = \frac{n!}{k!(n-k)!}
$$

For this distribution, $\mu_X = np$ and $\sigma^2_X = np(1-p)$. When n=1, this simplifies to the Bernoulli distribution.

### Challenge 4

What is the chance of getting a “1” on each of six consecutive rolls of a die? What about of getting exactly three “1”s? What is the expected number of “1”s to occur in six consecutive rolls?

```{r}
n <- 6  # number of trials
k <- 6  # number of successes
p <- 1/6
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -
    k)
prob

k <- 3  # number of successes
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -
    k)
prob
```

As for other distributions, ***R*** has a built in `density` function, the `dbinom()` function, that you can use to solve for the probability of a given outcome, i.e., *Pr*(X=x).

```{r}
dbinom(x=k, size=n, prob=p)
```

We can also use the built in function `pbinom()` to return the value of the **cumulative distribution function** for the binomial distribution, i.e., the probability of observing *up to and including* a given number of successes in n trials.

```{r}
probset <- dbinom(x = 0:6, size = 6, prob = 1/6)  # x is number of successes, size is number of trials
barplot(probset, names.arg = 0:6, space = 0, xlab = "outcome", ylab = "Pr(X = outcome)",
    main = "Probability Mass Function")
```

```{r}
cumprob = cumsum(probset)
barplot(cumprob, names.arg = 0:6, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
```

```{r}
sum(probset)
dbinom(x=3, size=6, prob=1/6)
pbinom(q=3, size=6, prob=1/6) # note the name of the argument is q not x

# which can also be calculated by summing the relevant individual outcome probabilities

sum(dbinom(x = 0:3, size = 6, prob = 1/6))  # this sums the probabilities of 0, 1, 2, and 3 successes

# the probability of observing more than 3 rolls of 1 is given as

1 - pnbinom(q = 3, size = 6, prob = 1/6)

# or

pnbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)

# the probability of obsercing 3 or more rolls of 1 is

1 - pbinom(q = 2, size = 6, prob = 1/6)  # note here that the q argument is '2'

# or 
pbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)
```

#### Poisson Distribution

The **Poisson Distribution** is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The *probability mass function* for the Poisson distribution is described by a single parameter, λ, where λ can be interpreted as the mean number of occurrences of the event in the given interval.

For the Poisson Distribution, the PMF is

$$
f(x) = \frac{\lambda^x e^{-\lambda}}{x!}
$$

Where x = {0, 1, 2, ...} and $\mu_X = \lambda$ and $\sigma^2_X = \lambda$

```{r}
x <- 0:10
l = 3.5
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

```{r}
x <- 0:20
l = 10
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

```{r}
x <- 0:50
l = 20
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

As we did for other distributions, we can also use the built in `probability` function for the Poisson distribution, `ppois()`, to return the value of the **cumulative distribution function**, i.e., the probability of observing up to and including a specific number of events in the given interval.

```{r}
x <- 0:10
l <- 3.5
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
```

```{r}
x <- 0:20
l <- 10
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
```

```{r}
x <- 0:50
l <- 20
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
```

#### Probability Density Functions

**The Uniform Distribution** - the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of *x* values in a given interval.

the PDF for the Uniform Distribution is:

$$
f(x) = \frac{1}{b-a}
$$

where $a \le x \le b$ and $0$ for $x \lt a$ and $x \gt b$

### Challenge 5

What would you predict the expectation (mean) should be for a uniform distribution?

for this distribution:

$$
\mu_X = \frac{a+b}{2} \\
\sigma^2_X = \frac{(b-a)^2}{12}
$$

Let's plot a uniform distribution across a given range from $a=4$ to $b=8$

```{r}
a <- 4
b <- 8
x <- seq(from = a - (b-a), to = b + (b-a), by = 0.01)
fx <- dunif(x, min = a, max = b) # dunif() evaluates the density at each x
plot(x, fx, type = "l", xlab = "x", ylab = "f(x)", main = "Probability Density Function")
```

```{r}
plot(x, punif(q=x, min=a, max=b), type="l", xlab="x", ylab="Pr(X ≤ x)", main = "Cumulative Probability") # punif() is the cumulative probability density up to a given x
```

### Challenge 6

Simulate a sample of 10000 random numbers from a uniform distribution in the interval between a = 6 and b = 8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.

```{r}
a <- 6
b <- 8
x <- runif(10000, min=a, max=b)
exmean <- (a + b)/2
exvar <- ((b-a)^2)/12
obmean <- mean(x)
obvar <- var(x)
print(paste0("Expected mean: ", exmean))
print(paste0("Observed mean: ", obmean))
print(paste0("Expected var: ", exvar))
print(paste0("Observed var: ", obvar))

```

**Normal (Gaussian) Distribution**

```{r}
mu <- 400
sigma <- 40
curve(dnorm(x, mu, sigma), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve",
    xlab = "x", ylab = "f(x)")
curve
```

![](images/clipboard-3942236420.png)

The code below lets you play interactively with μ, σ, and *nsigma* (which shades in the proportion of the distribution falling within that number of standard deviations of the mean). Also, look carefully at the code to try to figure out what each bit is doing.

```{r}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    polygon(rbind(c(mu - nsigma * sigma, 0), cbind(seq(from = (mu - nsigma *
        sigma), to = (mu + nsigma * sigma), length.out = 1000), dnorm(seq(from = (mu -
        nsigma * sigma), to = (mu + nsigma * sigma), length.out = 1000), mean = mu,
        sd = sigma)), c(mu + nsigma * sigma, 0)), border = NA, col = "salmon") +
    abline(v = mu, col = "blue") + abline(h = 0) + abline(v = c(mu - nsigma *
    sigma, mu + nsigma * sigma), col = "salmon"), mu = slider(-10, 10, initial = 9,
    step = 0.25), sigma = slider(0.25, 4, initial = 3, step = 0.25), nsigma = slider(0,
    4, initial = 1, step = 0.25))
```

The `pnorm()` function, as with the `p-` variant function for other distributions, returns the cumulative probability of observing a value less than or equal to x, i.e., *Pr* (X ≤ x). Type it the code below and then play with values of μ and σ to look at how the *cumulative distibution function* changes.

```{r}
manipulate(plot(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
    pnorm(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(-20, 20), xlab = "x", ylab = "f(x)",
    main = "Cumulative Probability"), mu = slider(-10, 10, initial = 0, step = 0.25),
    sigma = slider(0.25, 10, initial = 1, step = 0.25))  # plots the cumulative distribution function
```

You can also use `pnorm()` to calculate the probability of an observation drawn from the population falling within a particular interval. For example, for a normally distributed population variable with μ = 6 and σ = 2, the probability of a random observation falling between 7 and 8 is…

```{r}
p <- pnorm(8, mean = 6, sd = 2) - pnorm(7, mean = 6, sd = 2)
p
```

Likewise, you can use `pnorm()` to calculate the probability of an observation falling, for example within 2 standard deviations of the mean of a particular normal distribution.

```{r}
mu <- 0
sigma <- 1
p <- pnorm(mu + 2 * sigma, mean = mu, sd = sigma) - pnorm(mu - 2 * sigma, mean = mu,
    sd = sigma)
p
```

Regardless of the specific values of μ and σ, about 95% of the normal distribution falls within 2 standard deviations of the mean and about 68% of the distribution falls within 1 standard deviation.

```{r}
p <- pnorm(mu + 1 * sigma, mean = mu, sd = sigma) - pnorm(mu - 1 * sigma, mean = mu,
    sd = sigma)
p
```

Another one of the main functions in ***R*** for probability distributions, the `qnorm()` function, will tell us the value of x below which a given proportion of the cumulative probability function falls. As we saw earlier, too, we can use `qnorm()` to calculate confidence intervals. The code below

```{r}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    abline(v = mu, col = "blue") + abline(h = 0) + polygon(x = c(qnorm((1 -
    CI)/2, mean = mu, sd = sigma), qnorm((1 - CI)/2, mean = mu, sd = sigma),
    qnorm(1 - (1 - CI)/2, mean = mu, sd = sigma), qnorm(1 - (1 - CI)/2, mean = mu,
        sd = sigma)), y = c(0, 1, 1, 0), border = "red"), mu = slider(-10, 10,
    initial = 0, step = 0.25), sigma = slider(0.25, 10, initial = 1, step = 0.25),
    CI = slider(0.5, 0.99, initial = 0.9, step = 0.01))
```

### Challenge 7

-   Create a vector, *v*, containing **n** random numbers selected from a normal distribution with mean μ and standard deviation σ. Use 1000 for **n**, 3.5 for μ, and 4 for σ.

    -   **HINT:** Such a function exists! `rnorm()`.

-   Calculate the mean, variance, and standard deviation for your sample of random numbers.

-   Plot a histogram of your random numbers.

```{r}
# Creating the vector v
n <- 1000
mu <- 3.5
sigma <- 4
v <- rnorm(n, mu, sigma)

# Calculating the mean, 
mean(v)
# var, 
var(v)
# and sd.
sd(v)

# plotting
hist(v, breaks = seq(from=-15, to=20, by=0.5), probability = TRUE)
```

A quantile-quantile or “Q-Q” plot can be used to look at whether a set of data seem to follow a normal distribution. A Q–Q plot is a graphical method for generally comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data (as the y values) versus the theoretical quantiles (as the x values) pulled from a normal distribution. If the two distributions being compared are similar, the points in the plot will approximately lie on the line y = x.

In this case, this **should** be apparent since you have simulated a vector of data from a distribution normal distribution.

To quickly do a Q-Q plot, call the two ***R*** functions `qqnorm()` and `qqline()` using the vector of data you want to examine as an argument.

```{r}
qqnorm(v, main = "Normal QQ plot random normal variables")
qqline(v, col = "yellow")
```

This is the same as:

```{r}
# Step 1: generate a sequence of probability points in the interval from 0 to 1 equivalent in length to vector v

p <- ppoints(length(v))

# Step 2: Calculate the theoretical quantiles for this set of probabilities based on the distribution you want to compare to (in this case, the normal distribution)

theoretical_q <- qnorm(ppoints(length(v)))

# Step 3: Calculate the quantiles for your set of observed data for the same number of points

observed_q <- quantile(v, ppoints(v))

# Step 4: Plot these quantiles againsts one another

plot(theoretical_q, observed_q, main="Normal QQ plot ranodm normal variables", xlab="theoretical quantiles", ylab="Sample quantiles")
```

### Challenge 8

What happens if you simulate fewer observations in your vectors? Or if you simulate observations from a *different* distribution?

**The "Standard Normal" Distribution**

Any normal distribution with mean μ and standard deviation σ can be converted into what is called the **standard normal** distribution, where the mean is zero and the standard deviation is 1. This is done by subtracting the mean from all observations and dividing these differences by the standard deviation. The resultant values are referred to a *Z* scores, and they reflect the number of standard deviations an observation is from the mean.

```{r}
x <- rnorm(10000, mean=5, sd=8)
hist(x)
#mean(x)
#sd(x)
```

```{r}
z <- (x-mean(x))/sd(x)
hist(z)
```

#### Sample Distributions vs Population Distributions

 The most fundamental assumption is that the observations we make are *independent* from one another and are *identically distributed*, an assumption often abbreviated as **iid**. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption!

The important thing for us to know is that we can get unbiased estimates of population level parameters on the basis of sample statistics.

Let’s imagine a population of 1 million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years. Below, we set up our population:

```{r}
set.seed(1)
x <- rnorm(1e+06, 25, 5)
hist(x, probability=TRUE)
```

```{r}
mu <- mean(x)
sigma <- sqrt(sum((x-mean(x))^2)/length(x))
```

Suppose we now sample the zombie population by trapping sets of zombies and determining the mean age in each set. We sample without replacement from the original population for each set. Let’s do that 100 times with samples of size 5 and store these in a list.

```{r}
k <- 1000 # number of samples
n <- 5 # size of each sample
s <- NULL # dummy var to hold each sample
for(i in 1:k){
  s[[i]] <- sample(x, size=n, replace=FALSE)
}
head(s)
```

The sample mean - which is an average of the set of sample averages - is an unbiased estimator for the population mean.

```{r}
m <- NULL
for(i in 1:k){
  m[i] <- mean(s[[i]])
}
mean(m)
```

#### The Standard Error

The *variance in the sampling distribution*, i.e., of all possible means of samples of size *n* from a population, is $\sigma^2 /n$. The square root of this variance is the *standard deviation of the sampling distribution*, also referred to as the **standard error**.

Thus, if the population variance $\sigma^2$ (and thus the population standard deviation $\sigma$) is known, then the standard error can be calculated as **square root of** ( $\sigma^2 / n$ ) or, equivalently, $\sigma$**/(sqrt of sample size)**.

```{r}
pop_se <- sqrt(sigma^2/n)
#pop_se
```

```{r}
pop_se <- sigma/sqrt(n)
#pop_se
```

If the true population standard deviation is not known, the standard error can still be estimated from the standard deviation of any given sample. Thus, analogous to the formula we used when the true population standard deviation was known, the *standard error* calculated from a sample is simply the **sample standard deviation / (square root of the sample size)**, or…

```{r}
stdev <- NULL
for (i in 1:k) {
    stdev[i] <- sd(s[[i]])
}
sem <- stdev/sqrt(n)  # a vector of SEs estimated from each sample 
head(sem)
mean(sem)
pop_se
```

Thus, the *standard error of the mean* calculated from an individual sample can be used as an estimator for the **standard deviation of the sampling distribution**. This is extremely useful, since it means that, if our sample is large enough, we don’t have to repeatedly sample from the population to get an estimate of the sampling distribution directly using our data.

Note that as our sample size increases, the standard error of the mean should decrease, as should the standard deviation in estimates of the population mean drawn from successive samples. This should be apparent intuitively… as each sample drawn from a population gets larger, the estimate of the mean value of those samples should vary less and less.

Despite their similarities, the standard error of the mean calculated for a given sample and the standard deviation of that given sample tell us different things. The *standard error of the mean* is an estimate of how far a given sample mean is likely to be from the population mean; it is a measure of uncertainty. The *standard deviation* of a sample is a measure of the degree to which individuals values within a sample differ from the sample mean.
